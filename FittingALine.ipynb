{
 "metadata": {
  "name": "",
  "signature": "sha256:f187c867b20671be22143b6a15cece391bb0aac417f89fbfa0c1f6557640989d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequentism vs. Bayesianism II: Fitting a Line to Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a [previous post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) I gave a brief practical introduction to the frequentist and Bayesian approaches to statistics, covering both the philosophical underpinnings and practical aspects of these computations in Python. If you haven't yet read that post, please go read that first: most of what is below will assume that you've seen that previous material. Here, as promised, I'm going to explore a (slightly) more complicated model: the problem of fitting a straight line to data.\n",
      "\n",
      "You might laugh at me calling \"fitting a line to data\" a more complicated model. It isn't extremely complicated, true, but it is an excellend test bed to demonstrate the frequentist and Bayesian approaches to a general set of problems."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Easy Way"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course, simple straight-line regression is such a common problem that most statistics packages have a canned version built-in: one of the many ways to do this in Python is with [NumPy](http://numpy.org)'s ``polyfit`` function.  ``polyfit`` fits a polynomial to data, and a straight line is simply a degree-1 polynomial.\n",
      "\n",
      "First, though, we need to generate some data to try this on.  We'll start by defining a model and drawing some points from it with Gaussian errors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# define our model\n",
      "theta = [1, -5]  # [slope, intercept]\n",
      "def f(x, theta):\n",
      "    return theta[0] * x + theta[1]\n",
      "\n",
      "# Generate some data\n",
      "sigma = 1  # error on y\n",
      "np.random.seed(0)\n",
      "x = 10 * np.random.random(50)\n",
      "y = np.random.normal(f(x, theta), sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have some data to work with, we can use ``np.polyfit`` to find the line of best fit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit a line to the data\n",
      "theta_fit = np.polyfit(x, y, deg=1)\n",
      "print(theta_fit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visualize the data & fit\n",
      "xfit = np.linspace(0, 10)\n",
      "plt.plot(xfit, f(xfit, theta_fit))\n",
      "plt.errorbar(x, y, sigma, fmt='.k', ecolor='gray')\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('y');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Voil\u00e0! We have our best-fit line!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What Is This Function Doing?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When using a tool like ``np.polyfit``, it's rarely enough to simply apply the method and move on. To use a model like this in your research or data analysis, you really should make sure you know what's happening in the background, so that you understand the assumptions and approximations the model might entail.\n",
      "\n",
      "To this end, let's start from what we were looking at the [last post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/): the likelihood. For Gaussian measurement errors like we have here, the likelihood is simply the product of the normal-probability of each individual measurement:\n",
      "\n",
      "$$ \\mathcal{L}(\\theta~|~D) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left[\\frac{-[y_i - f(x_i;\\theta)]^2}{2\\sigma^2} \\right] $$\n",
      "\n",
      "This should be looking pretty familiar now. By taking the logarithm and ignoring constant terms, you can convince yourself that the likelihood $\\mathcal{L}$ is maximized when the following quantity is minimized:\n",
      "\n",
      "$$ S(\\theta) = \\sum_{i-1}^N [y_i - f(x_i;\\theta)]^2 $$\n",
      "\n",
      "Thus the optimal model is found by *minimizing the sum of square residuals*.  You might recognize this as the standard [least squares fitting](http://en.wikipedia.org/wiki/Least_squares) procedure, and now we see where this comes from: the least squares approach is derived from the maximum likelihood result!\n",
      "\n",
      "This minimization can be performed analytically in the case of a best-fit line (see this [Wolfram article](http://mathworld.wolfram.com/LeastSquaresFitting.html)), and can also be solved in general for any linear model, even if the errors $\\sigma_i$ are different for each point! But stepping through that derivation is not particularly enlightening, and the analytic approach doesn't easily generalize to more complex models. For those reasons, we'll skip the derivation and go straight to the numerical approach, using tools built-in to [SciPy](http://scipy.org) to find the optimal model for our data.\n",
      "\n",
      "Scipy's ``optimize`` module contains many useful routines for numerical optimization, one of which is the ``leastsq`` optimizer. The nice thing about this is that it can also return an estimate of the error on the final parameters, which essentially comes from fitting an N-dimensional Gaussian to the likelihood surface at maximum:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import leastsq\n",
      "theta_guess = [1, 1]\n",
      "\n",
      "def minfunc(theta):\n",
      "    return y - f(x, theta)\n",
      "\n",
      "print leastsq(minfunc, theta_guess)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comparing to the ``np.polyfit`` result above, we see that the answer is basically identical. This reflects the fact that ``polyfit`` is solving the exact same least-squares problem we wrote above; it's just taking a few extra steps to cast the minimization in terms of matrix algebra, which generally leads to a faster solution.\n",
      "\n",
      "The benefit of ``optimize.leastsq``, though, is that it can also quite quickly give you an estimate of the errors on your fit parameters. This is accomplished by asking the routine to return the ``full_output``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = leastsq(minfunc, theta_guess, full_output=True)\n",
      "mean = result[0]\n",
      "cov = result[1]\n",
      "\n",
      "print(\"best fit:\")\n",
      "print(mean)\n",
      "print(\"error covariance matrix:\")\n",
      "print cov"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The error is reported not as a single errorbar for each parameter, but as an *error covariance matrix*. The diagonal terms in this covariance matrix are related to the square of the typical error bars, and the off-diagonal terms give the covariance between these errors.\n",
      "\n",
      "This error covariance is often visualized as an ellipse, which we can create using a small utility function to convert the matrix to a tuple of $(r_1, r_2, \\alpha)$ which describes the ellipse (for background, see e.g. Section 3.5 in [our book](http://press.princeton.edu/titles/10159.html)). After defining this function, we'll plot the Ellipse representing the error bounds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_ellipse(M):\n",
      "    \"\"\"\n",
      "    Compute the rotation and principal axes of a 2D covariance matrix M\n",
      "    \"\"\"\n",
      "    alpha = 0.5 * np.arctan2(2 * M[1, 0], (M[0, 0] - M[1, 1]))\n",
      "    s1 = 0.5 * (M[0, 0] + M[1, 1])\n",
      "    s2 = np.sqrt(0.25 * (M[0, 0] - M[1, 1]) ** 2 + M[1, 0] ** 2)\n",
      "    return np.sqrt(s1 + s2), np.sqrt(s1 - s2), np.degrees(alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.patches import Ellipse\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "# plot a dot on the best-fit model\n",
      "ax.plot(mean[:1], mean[1:], 'ok')\n",
      "\n",
      "# plot cross-hairs on the true value\n",
      "ax.axvline(theta[0], color='lightgray')\n",
      "ax.axhline(theta[1], color='lightgray')\n",
      "\n",
      "# Plot 1-2-3 sigma ellipses\n",
      "r1, r2, alpha = get_ellipse(cov)\n",
      "for s in [1, 2, 3]:\n",
      "    #r1, r2 are radii: ellipse needs full widths 2*r1, 2*r2\n",
      "    ax.add_patch(Ellipse(mean, s * 2 * r1, s * 2 * r2, angle=alpha, fc='none'))\n",
      "\n",
      "ax.axis('auto')\n",
      "ax.set_xlabel('slope')\n",
      "ax.set_ylabel('intercept')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at this we see that there is a *correlation* between the slope and the intercept, and this correlation makes intuitive sense.  If you increase the slope of the line, you must decrease the intercept for it to fit the data, and vice versa.\n",
      "\n",
      "To make this more clear, let's plot a sampling of the possible lines covered by these ellipses. We can do this using the ``np.random.multivariate_normal`` function, which generates points from a multivariate normal distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_sample = np.random.multivariate_normal(mean, cov, 200)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(x, y, sigma, fmt='.k', ecolor='gray')\n",
      "for theta in theta_sample:\n",
      "    ax.plot(xfit, f(xfit, theta), '-b', lw=0.5, alpha=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This distribution of lines indicates the range of possible fits found through the least squares approach."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}